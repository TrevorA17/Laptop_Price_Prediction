---
title: "Laptop Price Prediction"
author: "Trevor Okinda"
date: "2024"
output:
  github_document: 
    toc: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    df_print: default
editor_options:
  chunk_output_type: console
---

# Student Details

|                                              |     |
|----------------------------------------------|-----|
| **Student ID Number**                        | 134780 |
| **Student Name**                             | Trevor Okinda |
| **BBIT 4.2 Group**                           | C |
| **Project Name**                             | Laptop Price Prediction |

# Setup Chunk

**Note:** the following KnitR options have been set as the global defaults: <BR> `knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, collapse = FALSE, tidy = TRUE)`.

More KnitR options are documented here <https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html> and here <https://yihui.org/knitr/options/>.

```{r setup, include=FALSE}
library(formatR)
knitr::opts_chunk$set(
  warning = FALSE,
  collapse = FALSE
)
```

### Source: 

The dataset that was used can be downloaded here: *\<https://www.kaggle.com/datasets/mrsimple07/laptoppriceprediction\>*

### Reference:

*\<Abdurakhimov, M. (2024). Laptop Price Prediction [Dataset]. Kaggle. https://www.kaggle.com/datasets/mrsimple07/laptoppriceprediction\>\
Refer to the APA 7th edition manual for rules on how to cite datasets: <https://apastyle.apa.org/style-grammar-guidelines/references/examples/data-set-references>*

# Understanding the Dataset (Exploratory Data Analysis (EDA))
## Loading dataset
```{r Load dataset}
# Load the required packages
library(dplyr)
library(psych)

# Load dataset
laptop_data <- read.csv("Laptop_price.csv", colClasses = c(
  Brand = "factor",
  Processor_Speed = "numeric",
  RAM_Size = "factor",
  Storage_Capacity = "factor",
  Screen_Size = "numeric",
  Weight = "numeric",
  Price = "numeric"
))

```

## Measures of Frequency
```{r MOF}
# Define a function to calculate mode
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}


# Measures of Frequency
# Frequency table for Brand
brand_freq <- table(laptop_data$Brand)
print("Frequency table for Brand:")
print(brand_freq)
```

## Measures of Central Tendency
```{r MOCT}
# Measures of Central Tendency
# Mean, Median, and Mode for Price
price_mean <- mean(laptop_data$Price)
price_median <- median(laptop_data$Price)
price_mode <- Mode(laptop_data$Price)
print("Measures of Central Tendency for Price:")
print(paste("Mean:", price_mean))
print(paste("Median:", price_median))
print(paste("Mode:", price_mode))
```

## Measures of Distribution
```{r MOD}
# Measures of Distribution
# Standard Deviation and Range for Screen Size
screen_sd <- sd(laptop_data$Screen_Size)
screen_range <- range(laptop_data$Screen_Size)
print("Measures of Distribution for Screen Size:")
print(paste("Standard Deviation:", screen_sd))
print(paste("Range:", screen_range))
```

## Measures of Relationship
```{r MOR}
# Measures of Relationship
# Correlation between Processor Speed and Price
processor_price_corr <- cor(laptop_data$Processor_Speed, laptop_data$Price)
print("Correlation between Processor Speed and Price:")
print(processor_price_corr)

# Correlation between RAM Size and Price
# Since RAM Size is currently a factor, we need to convert it to numeric first
laptop_data$RAM_Size <- as.numeric(as.character(laptop_data$RAM_Size))
ram_price_corr <- cor(laptop_data$RAM_Size, laptop_data$Price)
print("Correlation between RAM Size and Price:")
print(ram_price_corr)
```

## ANOVA
```{r ANOVA}
# Load the required packages
library(dplyr)

# Convert RAM_Size and Storage_Capacity to factors
laptop_data$RAM_Size <- as.factor(laptop_data$RAM_Size)
laptop_data$Storage_Capacity <- as.factor(laptop_data$Storage_Capacity)

# Perform ANOVA on Brand
anova_brand <- aov(Price ~ Brand, data = laptop_data)
print(summary(anova_brand))

# Post-hoc tests for pairwise comparisons
pairwise_tukey <- TukeyHSD(anova_brand)
print(pairwise_tukey)

```

## Plots
```{r Plots}
library(ggplot2)
# Univariate Plots
# Histogram for Price
histogram_price <- ggplot(laptop_data, aes(x = Price)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 20) +
  labs(title = "Histogram of Laptop Prices", x = "Price", y = "Frequency")
print(histogram_price)

# Boxplot for Screen Size
boxplot_screen_size <- ggplot(laptop_data, aes(x = "", y = Screen_Size)) +
  geom_boxplot(fill = "lightgreen", color = "black") +
  labs(title = "Boxplot of Screen Size", x = "", y = "Screen Size")
print(boxplot_screen_size)

# Multivariate Plot
# Scatter plot of Price vs. Processor Speed
scatter_price_processor <- ggplot(laptop_data, aes(x = Processor_Speed, y = Price)) +
  geom_point(color = "blue") +
  labs(title = "Scatter Plot of Price vs. Processor Speed", x = "Processor Speed", y = "Price")
print(scatter_price_processor)


```

# Preprocessing and Data Transformation
## Missing Values
```{r Missing Values}
# Check for NA values in the dataset
na_count <- sum(is.na(laptop_data))
print(paste("Number of NA values in the dataset:", na_count))

# Check for NULL values in the dataset
null_count <- sum(is.null(laptop_data))
print(paste("Number of NULL values in the dataset:", null_count))

# Check for missing values in specific columns
missing_values_brand <- sum(is.na(laptop_data$Brand))
print(paste("Number of missing values in Brand column:", missing_values_brand))

missing_values_price <- sum(is.na(laptop_data$Price))
print(paste("Number of missing values in Price column:", missing_values_price))

```

# Model Training
## Data Splitting
```{r Data Splitting}
library(caret)

# Data Splitting
set.seed(123) # for reproducibility
train_index <- createDataPartition(laptop_data$Price, p = 0.8, list = FALSE)
train_data <- laptop_data[train_index, ]
test_data <- laptop_data[-train_index, ]

dim(train_data)
dim(test_data)
```

## Bootstrapping
```{r Bootstrapping}
# Bootstrapping
bootstrapped_means <- replicate(1000, mean(sample(train_data$Price, replace = TRUE)))
bootstrapped_sd <- sd(bootstrapped_means)
print(paste("Bootstrapped Standard Deviation of means:", bootstrapped_sd))
```

## Cross-validation
```{r Cross-validation}
# Cross-validation
set.seed(123) # for reproducibility
fit_control <- trainControl(method = "cv", number = 5)
model <- train(Price ~ ., data = train_data, method = "lm", trControl = fit_control)
print(summary(model))
print(model)
```

## Training Different Models
```{r Training Different Models}
# Load the required packages
library(caret)
library(rpart)
library(randomForest)

# Data Splitting
set.seed(123) # for reproducibility
train_index <- createDataPartition(laptop_data$Price, p = 0.8, list = FALSE)
train_data <- laptop_data[train_index, ]
test_data <- laptop_data[-train_index, ]

# Linear Regression (lm)
lm_model <- lm(Price ~ ., data = train_data)
print(summary(lm_model))

# Decision Tree
tree_model <- rpart(Price ~ ., data = train_data)
print(tree_model)



```

## Saving Model
```{r Saving Model}
# Load the saved model
loaded_lm_model <- readRDS("./models/saved_lm_model.rds")

# Prepare new data for prediction
new_data <- data.frame(
  Brand = factor("Asus"),
  Processor_Speed = as.numeric(3.0),
  RAM_Size = factor("8"),
  Storage_Capacity = factor("512"),
  Screen_Size = as.numeric(15.6),
  Weight = as.numeric(3.5)
)

# Use the loaded model to make predictions
predictions_loaded_model <- predict(loaded_lm_model, newdata = new_data)

# Print predictions
print(predictions_loaded_model)

```

